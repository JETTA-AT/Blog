<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jet Smith</title>
  
  
  <link href="/Blog/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-21T10:23:05.207Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jet Smith</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Kafka权威指南》小结 NO.1</title>
    <link href="http://yoursite.com/2019/03/10/%E3%80%8AKafka%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8B%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2019/03/10/《Kafka权威指南》小结/</id>
    <published>2019-03-10T10:32:34.000Z</published>
    <updated>2019-03-21T10:23:05.207Z</updated>
    
    <content type="html"><![CDATA[<img src="/Blog/2019/03/10/《Kafka权威指南》小结/20190321181123.png" title="《Kafka权威指南》小结"><h1 id="Kafka权威指南"><a href="#Kafka权威指南" class="headerlink" title="Kafka权威指南"></a>Kafka权威指南</h1><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><h3 id="不同于消息系统"><a href="#不同于消息系统" class="headerlink" title="不同于消息系统"></a>不同于消息系统</h3><ul><li>集群方式运行，自由伸缩</li><li>数据连接层：保证数据传递–复制、持久化<h3 id="更像是实时版的hadoop"><a href="#更像是实时版的hadoop" class="headerlink" title="更像是实时版的hadoop"></a>更像是实时版的hadoop</h3></li><li>流式处理可以是批处理的超集，差异是延时</li><li>hadoop和大数据应用在数据分析</li><li>kafka更适合在核心业务上，快速响应提升用户体验</li><li>概要: 架构设计<h3 id="与ETL工具比较"><a href="#与ETL工具比较" class="headerlink" title="与ETL工具比较"></a>与ETL工具比较</h3></li><li>都擅长移动数据，但kafka颠覆了传统思维</li><li>并不仅仅吧数据从一个系统拆解，塞入另一个系统</li><li>不仅将应用和数据连接，还强化了相同数据流的应用</li><li>概要: 数据流为核心，和现金流一样重要<h2 id="1-初识Kafka"><a href="#1-初识Kafka" class="headerlink" title="1.初识Kafka"></a>1.初识Kafka</h2><h3 id="1-1-发布与订阅消息系统"><a href="#1-1-发布与订阅消息系统" class="headerlink" title="1.1 发布与订阅消息系统"></a>1.1 发布与订阅消息系统</h3></li><li>单个直连的度量指标发布者</li><li>多个个直连的度量指标发布者<ul><li>单独获取，节点间的连接一团糟</li></ul></li><li>度量指标发布与订阅系统<ul><li>类似总线，通过统一管道，满足不同需求</li></ul></li><li>多个发布与订阅系统<ul><li>维护多个数据队列，每个系统有各有缺陷</li><li>你需要一个单一集中式的系统<h3 id="1-2-Kafka登场"><a href="#1-2-Kafka登场" class="headerlink" title="1.2 Kafka登场"></a>1.2 Kafka登场</h3></li></ul></li><li>1.2.1 消息和批次<ul><li>消息有一个可选元数据—-键（一个字节数组）<ul><li>当消息以一种可控方式写入不同分区时，会用到键</li><li>为键生成一个一致性散列值，用散列值对主题分区进行取模，这样可以保证具有相同键的消息总被写在相同分区上</li></ul></li><li>批次是一组消息<ul><li>用于减少网络开销，要在时间延迟和吞吐量上做出权衡</li><li>批次会被压缩，提升传输存储能力，但会做更多计算处理</li></ul></li></ul></li><li>1.2.2 模式<ul><li>消息模式<ul><li>相比JSON XML这类系统，不仅易用，而且可读性好</li><li>但缺乏强类型处理能力，版本间 兼容性不是很好</li></ul></li><li>数据格式一致性，消除消息的读写操作之间的耦合性<ul><li>不需要订阅者升级后，发布者才能跟着升级</li></ul></li></ul></li><li>1.2.3 主题和分区<ul><li>消息在topic里不能保证顺序，但单个分区里是顺序的</li><li>Kafka通过分区来实现数据冗余和伸缩性</li><li>流是一组从producer移动到consumer的数据</li></ul></li><li>1.2.4 producer and consumer<ul><li>producer<ul><li>producer在默认的情况下把消息均衡地分布到topic的所有分区上，而不关心特定消息被写到那个partation</li><li>生产者会把消息写到指定分区，通常使用消息键和分区器实现，分区器是键生成的散列值。</li></ul></li><li>consumer<ul><li>消费者通过检查消息的偏移量来区分是否读已取过<ul><li>偏移量是另一种元数据</li></ul></li><li>消费者是消费者群组的一部分<ul><li>消费者于分区之间映射：所有权关系</li></ul></li></ul></li></ul></li><li>1.2.5 broker 和集群<ul><li>broker<ul><li>一个独立的kafka服务器被称为broker<ul><li>接收producer消息，为消息设置偏移量，提交消息到磁盘</li><li>单个broker可以轻松处理千个分区、每秒百万级消息量</li></ul></li><li>每个集群都有一个broker 同时 充当了–集群控制器<ul><li>一个分区从属一个broker，该broker被称为分区的首领</li><li>一个分区分给多个个broker：复制</li></ul></li><li>保留消息<ul><li>保留一段时间，保留一定大小的字节数</li></ul></li></ul></li></ul></li><li>1.2.6 多集群<ul><li>集群间的消息复制：MirrorMaker</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;/Blog/2019/03/10/《Kafka权威指南》小结/20190321181123.png&quot; title=&quot;《Kafka权威指南》小结&quot;&gt;
&lt;h1 id=&quot;Kafka权威指南&quot;&gt;&lt;a href=&quot;#Kafka权威指南&quot; class=&quot;headerlin
      
    
    </summary>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>浅析ACID、BASE、CAP原理</title>
    <link href="http://yoursite.com/2019/03/03/%E6%B5%85%E6%9E%90ACID%E3%80%81BASE%E3%80%81CAP%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/03/03/浅析ACID、BASE、CAP原理/</id>
    <published>2019-03-03T08:56:37.000Z</published>
    <updated>2019-03-21T09:11:22.074Z</updated>
    
    <content type="html"><![CDATA[<img src="/Blog/2019/03/03/浅析ACID、BASE、CAP原理/20190321170312.png" title="浅析ACID、BASE、CAP原理"><h3 id="浅析ACID、BASE、CAP原理"><a href="#浅析ACID、BASE、CAP原理" class="headerlink" title="浅析ACID、BASE、CAP原理"></a>浅析ACID、BASE、CAP原理</h3><ul><li>ACID保证永久更新数据库<ul><li>原子性Atomic<ul><li>不可分割，只有全部成功，才算成功</li></ul></li><li>一致性Consistency<ul><li>不能破坏业务逻辑上的一致性，转账存款总额不变</li><li>数据库的约束 级联和Trigger都必须满足事务的一致性</li></ul></li><li>隔离性Isolation<ul><li>并发环境中，各事务都有各自完整数据空间<ul><li>事务锁</li></ul></li></ul></li><li>持久性Durability<ul><li>事务结束，数据库更改必须永久保存，重启依旧恢复事务成功结束状态</li></ul></li></ul></li><li>CAP理论<ul><li>一致性Consistency<ul><li>同样数据在分布式系统中所有地方都是被复制成相同</li></ul></li><li>可用性Availability<ul><li>所有在分布式系统活跃的节点都能处理操作且能响应查询</li><li>可用并不意味着数据的一致，比如过期的数据或脏读</li></ul></li><li>分区容错性Partition tolerancce<ul><li>除整个网络故障外，均导致整个系统无法正确响应</li></ul></li><li>理论：一个分布式系统最多只能同时满足三项中的两项</li></ul></li><li>BASE理论<ul><li>核心思想<ul><li>基于CAP理论何其基础上的延伸出来的 BASE 理论，有人提出柔性事务概念</li><li>即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性</li><li>与 ACID 相比，以牺牲强一致性获得高可用性</li></ul></li><li>基本可用 Basically Available<ul><li>核心可用，系统故障允许损失大部分可用性</li><li>降级服务，应对访问量激增，屏蔽一些功能</li></ul></li><li>柔性状态 Soft State<ul><li>允许中间状态，并不影响整体可用性<ul><li>例如三个副本，允许不同节点件的副本同步延迟时</li></ul></li></ul></li><li>最终一致性 Eventual Consitency<ul><li>副本经过一定时间后，最终达到一致状态</li></ul></li></ul></li><li>后记<ul><li>单机系统总线不会丢数据，而网络会。机器间的通讯，可能是收到，未收到，不知道收没收到，同步状态成本很高<ul><li>Paxos 确保一致性，但只有到一半以上确认后才能确认成功，这种强同步不是最终一致性</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;/Blog/2019/03/03/浅析ACID、BASE、CAP原理/20190321170312.png&quot; title=&quot;浅析ACID、BASE、CAP原理&quot;&gt;
&lt;h3 id=&quot;浅析ACID、BASE、CAP原理&quot;&gt;&lt;a href=&quot;#浅析ACID、BASE
      
    
    </summary>
    
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【转】Spark动态设置 Shuffle Partition</title>
    <link href="http://yoursite.com/2019/03/01/%E3%80%90%E8%BD%AC%E3%80%91Spark%E5%8A%A8%E6%80%81%E8%AE%BE%E7%BD%AE%20Shuffle%20Partition/"/>
    <id>http://yoursite.com/2019/03/01/【转】Spark动态设置 Shuffle Partition/</id>
    <published>2019-03-01T09:21:37.000Z</published>
    <updated>2019-03-22T03:13:54.772Z</updated>
    
    <content type="html"><![CDATA[<p>文章转自: <a href="http://www.jasongj.com/spark/adaptive_execution/" target="_blank" rel="noopener">Adaptive Execution 让 Spark SQL 更高效更智能</a></p><p>动态设置 Shuffle Partition<br>2.1 Spark Shuffle 原理<br>Spark Shuffle 一般用于将上游 Stage 中的数据按 Key 分区，保证来自不同 Mapper （表示上游 Stage 的 Task）的相同的 Key 进入相同的 Reducer （表示下游 Stage 的 Task）。一般用于 group by 或者 Join 操作。</p><img src="/Blog/2019/03/01/【转】Spark动态设置%20Shuffle%20Partition/spark_ae_fix_reducer_detail.png" title="【转】Spark动态设置 Shuffle Partition"><p>如上图所示，该 Shuffle 总共有 2 个 Mapper 与 5 个 Reducer。每个 Mapper 会按相同的规则（由 Partitioner 定义）将自己的数据分为五份。每个 Reducer 从这两个 Mapper 中拉取属于自己的那一份数据。<br>2.2 原有 Shuffle 的问题<br>使用 Spark SQL 时，可通过 spark.sql.shuffle.partitions 指定 Shuffle 时 Partition 个数，也即 Reducer 个数<br>该参数决定了一个 Spark SQL Job 中包含的所有 Shuffle 的 Partition 个数。如下图所示，当该参数值为 3 时，所有 Shuffle 中 Reducer 个数都为 3</p><img src="/Blog/2019/03/01/【转】Spark动态设置%20Shuffle%20Partition/spark_ae_fix_reducer.png" title="【转】Spark动态设置 Shuffle Partition"><p>这种方法有如下问题<br>    • Partition 个数不宜设置过大<br>        ○ Reducer（代指 Spark Shuffle 过程中执行 Shuffle Read 的 Task） 个数过多，每个 Reducer 处理的数据量过小。大量小 Task 造成不必要的 Task 调度开销与可能的资源调度开销（如果开启了 Dynamic Allocation）<br>        ○ Reducer 个数过大，如果 Reducer 直接写 HDFS 会生成大量小文件，从而造成大量 addBlock RPC，Name node 可能成为瓶颈，并影响其它使用 HDFS 的应用<br>        ○ 过多 Reducer 写小文件，会造成后面读取这些小文件时产生大量 getBlock RPC，对 Name node 产生冲击<br>    • Partition 个数不宜设置过小<br>        ○ 每个 Reducer 处理的数据量太大，Spill 到磁盘开销增大<br>        ○ Reducer GC 时间增长<br>        ○ Reducer 如果写 HDFS，每个 Reducer 写入数据量较大，无法充分发挥并行处理优势<br>    • 很难保证所有 Shuffle 都最优<br>        ○ 不同的 Shuffle 对应的数据量不一样，因此最优的 Partition 个数也不一样。使用统一的 Partition 个数很难保证所有 Shuffle 都最优<br>        ○ 定时任务不同时段数据量不一样，相同的 Partition 数设置无法保证所有时间段执行时都最优<br>2.3 自动设置 Shuffle Partition 原理<br>如 Spark Shuffle 原理 一节图中所示，Stage 1 的 5 个 Partition 数据量分别为 60MB，40MB，1MB，2MB，50MB。其中 1MB 与 2MB 的 Partition 明显过小（实际场景中，部分小 Partition 只有几十 KB 及至几十字节）<br>开启 Adaptive Execution 后<br>    • Spark 在 Stage 0 的 Shuffle Write 结束后，根据各 Mapper 输出，统计得到各 Partition 的数据量，即 60MB，40MB，1MB，2MB，50MB<br>    • 通过 ExchangeCoordinator 计算出合适的 post-shuffle Partition 个数（即 Reducer）个数（本例中 Reducer 个数设置为 3）<br>    • 启动相应个数的 Reducer 任务<br>    • 每个 Reducer 读取一个或多个 Shuffle Write Partition 数据（如下图所示，Reducer 0 读取 Partition 0，Reducer 1 读取 Partition 1、2、3，Reducer 2 读取 Partition 4）</p><img src="/Blog/2019/03/01/【转】Spark动态设置%20Shuffle%20Partition/spark_ae_auto_reducer_detail_1.png" title="【转】Spark动态设置 Shuffle Partition"><p>三个 Reducer 这样分配是因为<br>    • targetPostShuffleInputSize 默认为 64MB，每个 Reducer 读取数据量不超过 64MB<br>    • 如果 Partition 0 与 Partition 2 结合，Partition 1 与 Partition 3 结合，虽然也都不超过 64 MB。但读完 Partition 0 再读 Partition 2，对于同一个 Mapper 而言，如果每个 Partition 数据比较少，跳着读多个 Partition 相当于随机读，在 HDD 上性能不高<br>    • 目前的做法是只结合相临的 Partition，从而保证顺序读，提高磁盘 IO 性能<br>    • 该方案只会合并多个小的 Partition，不会将大的 Partition 拆分，因为拆分过程需要引入一轮新的 Shuffle<br>    • 基于上面的原因，默认 Partition 个数（本例中为 5）可以大一点，然后由 ExchangeCoordinator 合并。如果设置的 Partition 个数太小，Adaptive Execution 在此场景下无法发挥作用<br>由上图可见，Reducer 1 从每个 Mapper 读取 Partition 1、2、3 都有三根线，是因为原来的 Shuffle 设计中，每个 Reducer 每次通过 Fetch 请求从一个特定 Mapper 读数据时，只能读一个 Partition 的数据。也即在上图中，Reducer 1 读取 Mapper 0 的数据，需要 3 轮 Fetch 请求。对于 Mapper 而言，需要读三次磁盘，相当于随机 IO。<br>为了解决这个问题，Spark 新增接口，一次 Shuffle Read 可以读多个 Partition 的数据。如下图所示，Task 1 通过一轮请求即可同时读取 Task 0 内 Partition 0、1 和 2 的数据，减少了网络请求数量。同时 Mapper 0 一次性读取并返回三个 Partition 的数据，相当于顺序 IO，从而提升了性能。</p><p>由于 Adaptive Execution 的自动设置 Reducer 是由 ExchangeCoordinator 根据 Shuffle Write 统计信息决定的，因此即使在同一个 Job 中不同 Shuffle 的 Reducer 个数都可以不一样，从而使得每次 Shuffle 都尽可能最优。<br>上文 原有 Shuffle 的问题 一节中的例子，在启用 Adaptive Execution 后，三次 Shuffle 的 Reducer 个数从原来的全部为 3 变为 2、4、3。</p><img src="/Blog/2019/03/01/【转】Spark动态设置%20Shuffle%20Partition/spark_ae_fix_reducer.png" title="【转】Spark动态设置 Shuffle Partition"><p>2.4 使用与优化方法<br>可通过 spark.sql.adaptive.enabled=true 启用 Adaptive Execution 从而启用自动设置 Shuffle Reducer 这一特性<br>通过 spark.sql.adaptive.shuffle.targetPostShuffleInputSize 可设置每个 Reducer 读取的目标数据量，其单位是字节，默认值为 64 MB。上文例子中，如果将该值设置为 50 MB，最终效果仍然如上文所示，而不会将 Partition 0 的 60MB 拆分。具体原因上文已说明</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;文章转自: &lt;a href=&quot;http://www.jasongj.com/spark/adaptive_execution/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Adaptive Execution 让 Spark SQL 更高效更智能&lt;/a&gt;
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>《企业IT架构转型之道》小结 No.1</title>
    <link href="http://yoursite.com/2019/02/27/%E3%80%8A%E4%BC%81%E4%B8%9AIT%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%9E%8B%E4%B9%8B%E9%81%93%E3%80%8B%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2019/02/27/《企业IT架构转型之道》小结/</id>
    <published>2019-02-27T08:56:37.000Z</published>
    <updated>2019-03-21T10:23:12.358Z</updated>
    
    <content type="html"><![CDATA[<img src="/Blog/2019/02/27/《企业IT架构转型之道》小结/20190321171954.png" title="《企业IT架构转型之道》小结"><h1 id="企业IT架构转型之道"><a href="#企业IT架构转型之道" class="headerlink" title="企业IT架构转型之道"></a>企业IT架构转型之道</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="大数据平台"><a href="#大数据平台" class="headerlink" title="大数据平台"></a>大数据平台</h3><ul><li>数据层访问打通，数据权限的控制。</li><li>数据格式的转换，数据清洗，数据同步。<h3 id="人的配置"><a href="#人的配置" class="headerlink" title="人的配置"></a>人的配置</h3></li><li>缺少能基于数据有业务建模能力的专家</li><li>数据采集，数学算法，数学软件，数据分析，预测分析，市场应用，决策分析<h3 id="人的发展"><a href="#人的发展" class="headerlink" title="人的发展"></a>人的发展</h3></li><li>烟囱式系统建设，不同的角色技术人员很难对某一业务领域有持续的理解和沉淀。</li><li>而采用环绕服务能力持续运营构建独立组织的形态，懂技术懂业务的复合型人才<h2 id="共享服务架构选择-amp-建设原则"><a href="#共享服务架构选择-amp-建设原则" class="headerlink" title="共享服务架构选择&amp;建设原则"></a>共享服务架构选择&amp;建设原则</h2><h3 id="企业服务总线（ESB）-实现SOA方案"><a href="#企业服务总线（ESB）-实现SOA方案" class="headerlink" title="企业服务总线（ESB） 实现SOA方案"></a>企业服务总线（ESB） 实现SOA方案</h3></li><li>HSF<ul><li>每一个HSF应用均是以War包形式存在，运行在Tomcat容器，Tomcat容器层集成HSF服务框架，无需引入Jar</li><li>Nginx提供服务器和Diamond的列表信息</li><li>HSF框架采用Netty+Hession 数据序列化协议实现服务交互</li><li>这类RPC协议采用多路复用的TCP长连接，多个服务请求调用同一个长连接</li><li>TPS上达到10w性能远比 REST 或 Web Service 高<h3 id="共享服务中心"><a href="#共享服务中心" class="headerlink" title="共享服务中心"></a>共享服务中心</h3></li></ul></li><li>设计</li><li>运营</li><li>工程</li><li>概要: 原则<ul><li>高内聚、低耦合<ul><li>业务之间依赖性很大，但服务间隔离性很大</li></ul></li><li>数据完整性</li><li>业务可运营性原则（巡检）<ul><li>业务逻辑，数据沉淀，产生价值</li></ul></li><li>渐进性的建设原则<h2 id="数据库能力线性扩展"><a href="#数据库能力线性扩展" class="headerlink" title="数据库能力线性扩展"></a>数据库能力线性扩展</h2><h3 id="跨库查询"><a href="#跨库查询" class="headerlink" title="跨库查询"></a>跨库查询</h3></li></ul></li><li>分布式数据层框架TDDL（数据中间件）<ul><li>Matrix层（TDataSourcre）实现分库分表逻辑，底下持有多个GroupDs实例</li><li>Group层（TGroupSource）实现数据库的主备/读写分离逻辑，底层持有多个AtomDs实例</li><li>Atom层（TAtomDataSource）实现数据库连接等信息的动态推送，持有原子的数据源<h2 id="异步化与缓存原则"><a href="#异步化与缓存原则" class="headerlink" title="异步化与缓存原则"></a>异步化与缓存原则</h2><h3 id="数据库事务异步化（将大事务拆成小事务，降低锁占用）"><a href="#数据库事务异步化（将大事务拆成小事务，降低锁占用）" class="headerlink" title="数据库事务异步化（将大事务拆成小事务，降低锁占用）"></a>数据库事务异步化（将大事务拆成小事务，降低锁占用）</h3></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;/Blog/2019/02/27/《企业IT架构转型之道》小结/20190321171954.png&quot; title=&quot;《企业IT架构转型之道》小结&quot;&gt;
&lt;h1 id=&quot;企业IT架构转型之道&quot;&gt;&lt;a href=&quot;#企业IT架构转型之道&quot; class=&quot;heade
      
    
    </summary>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Spark函数:浅析combineByKey、reduceByKey、groupByKey</title>
    <link href="http://yoursite.com/2019/02/21/%E8%AE%A4%E7%9F%A5combineByKey%E3%80%81reduceByKey%E3%80%81groupByKey/"/>
    <id>http://yoursite.com/2019/02/21/认知combineByKey、reduceByKey、groupByKey/</id>
    <published>2019-02-21T08:56:37.000Z</published>
    <updated>2019-03-21T08:40:01.469Z</updated>
    
    <content type="html"><![CDATA[<p>简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。</p><h2 id="combineByKey函数"><a href="#combineByKey函数" class="headerlink" title="combineByKey函数"></a>combineByKey函数</h2><pre><code class="bash">def combineByKey[C](        createCombiner: V =&gt; C,        mergeValue: (C, V) =&gt; C,        mergeCombiners: (C, C) =&gt; C,        partitioner: Partitioner,        mapSideCombine: Boolean = <span class="literal">true</span>,        serializer: Serializer = null)</code></pre><h3 id="三个自定义方法"><a href="#三个自定义方法" class="headerlink" title="三个自定义方法"></a>三个自定义方法</h3><h4 id="1-createCombiner"><a href="#1-createCombiner" class="headerlink" title="1.createCombiner"></a>1.createCombiner</h4><p>这个函数把当前rdd中的值（value）作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作，分区内操作)</p><h4 id="2-mergeValue"><a href="#2-mergeValue" class="headerlink" title="2. mergeValue"></a>2. mergeValue</h4><p>该函数把元素V合并到之前的元素C(createCombiner)上 (每个分区内合并)</p><h4 id="3-mergeCombiner"><a href="#3-mergeCombiner" class="headerlink" title="3. mergeCombiner"></a>3. mergeCombiner</h4><p>该函数把2个元素C合并 (此函数作用范围在rdd的不同分区间内，跨分区合并)</p><h3 id="使用combineByKey求平均数"><a href="#使用combineByKey求平均数" class="headerlink" title="使用combineByKey求平均数"></a>使用combineByKey求平均数</h3><p>测试代码如下：</p><pre><code class="bash">import org.apache.spark.{SparkConf, SparkContext}object combineByKey {  def main(args: Array[String]): Unit = {    val sparkConf = new SparkConf()      .<span class="built_in">set</span>(<span class="string">"spark.io.compression.codec"</span>, <span class="string">"snappy"</span>)      .setAppName(<span class="string">"dsp_get_request"</span>).setMaster(<span class="string">"local[*]"</span>)    val sc = new SparkContext(sparkConf)    val initialScores = Array((<span class="string">"Fred"</span>, 88.0), (<span class="string">"Fred"</span>, 95.0), (<span class="string">"Fred"</span>, 91.0), (<span class="string">"Wilma"</span>, 93.0), (<span class="string">"Wilma"</span>, 95.0), (<span class="string">"Wilma"</span>, 98.0))    val d1 = sc.parallelize(initialScores)    <span class="built_in">type</span> MVType = (Int, Double) //定义一个元组类型(科目计数器,分数)    val rdd2 = d1.combineByKey(      score =&gt; (1,score),  // 将score映射为一个元组，作为分区内聚合初始值      (c1: MVType , newscore) =&gt; (c1._1 + 1 , c1._2 + newscore),  //分区内聚合      (c1: MVType ,c2: MVType) =&gt; (c1._1 + c2._1,c1._2 + c2._2)  //分区间聚合    ).map{ <span class="keyword">case</span>(name ,(num ,cnt)) =&gt; (name ,num / cnt)}    rdd2.foreach(println)  }}--------------------------------------------------(Fred,91.33333333333333)(Wilma,95.33333333333333)</code></pre><h4 id="使用reduceByKey求平均数"><a href="#使用reduceByKey求平均数" class="headerlink" title="使用reduceByKey求平均数"></a>使用reduceByKey求平均数</h4><p>现在用reduceByKey改写刚才的平均数算法</p><pre><code class="bash">val rdd3 = d1.map(a =&gt; (a._1, (a._2, 1)))  .reduceByKey((a,b) =&gt; (a._1+b._1,a._2+b._2))  .map(t =&gt; (t._1,t._2._1/t._2._2))rdd3.foreach(println)</code></pre><h2 id="reduceByKey、groupByKey"><a href="#reduceByKey、groupByKey" class="headerlink" title="reduceByKey、groupByKey"></a>reduceByKey、groupByKey</h2><p>我们再看看reduceByKey、groupByKey的区别：</p><pre><code class="bash">d1.foreach(println)------------------------(Wilma,93.0)(Fred,95.0)(Fred,88.0)(Fred,91.0)(Wilma,95.0)(Wilma,98.0)d1.reduceByKey(_ + _).foreach(println)-------------------------(Wilma,286.0)(Fred,274.0)d1.groupByKey().foreach(println)-------------------------(Wilma,CompactBuffer(93.0, 95.0, 98.0))(Fred,CompactBuffer(88.0, 95.0, 91.0))</code></pre><h3 id="reduceByKey效率会更高"><a href="#reduceByKey效率会更高" class="headerlink" title="reduceByKey效率会更高"></a>reduceByKey效率会更高</h3><p>其中reduceByKey(_ + _)写法同等于reduceByKey((a,b) =&gt; (a + b))<br>groupByKey会把所有的键值对集合都加载到内存中存储计算，若一个键值对太多，则会导致内存溢出。<br>reduceByKey 进行shuffle之前会在map做合并，这样就会减少shuffle 的IO 传送，效率会高一些。</p><h3 id="groupByKey聚合"><a href="#groupByKey聚合" class="headerlink" title="groupByKey聚合"></a>groupByKey聚合</h3><pre><code class="bash">d1.groupByKey().map(t =&gt; (t._1,t._2.sum)).foreach(println)-------------------------(Fred,274.0)(Wilma,286.0)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。&lt;/p&gt;
&lt;h2 id=&quot;combineB
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark函数:浅析SparkSession</title>
    <link href="http://yoursite.com/2019/02/18/%E6%B5%85%E6%9E%90SparkSession/"/>
    <id>http://yoursite.com/2019/02/18/浅析SparkSession/</id>
    <published>2019-02-18T09:21:37.000Z</published>
    <updated>2019-03-22T07:09:35.196Z</updated>
    
    <content type="html"><![CDATA[<p>在理解spark-session之前让我们理解入口点，一个入口点是控制从操作系统传递到提供的程序的地方。<br>在2.0入口之前，spark-core是sparkContext.Apache Spark是一个功能强大的集群计算引擎，因此它专为快速计算大数据而设计。<br>而这个入口可就是SparkContext。</p><h2 id="SparkContext在Apache-Spark中的功能："><a href="#SparkContext在Apache-Spark中的功能：" class="headerlink" title="SparkContext在Apache Spark中的功能："></a>SparkContext在Apache Spark中的功能：</h2><p>1.获取spark应用程序的当前状态<br>2.设置配置<br>3.访问各种服务<br>4.取消job<br>5.取消一个stage<br>6.关闭清洁<br>7.注册Spark-Listener<br>8.可编程动态分配<br>9.访问持久性RDD</p><p>SparkConf是创建spark上下文对象所必需的，它存储配置参数，如appName（用于标识spark驱动程序），core的数目和在工作节点上运行的执行程序的内存大小。</p><pre><code class="scala"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()    <span class="comment">//core数目 local 本地单核/ local[2] 本地2核 / local[*]  本地全部核心</span>  .setMaster(<span class="string">"local"</span>)    .setAppName(<span class="string">"Spark Practice"</span>)<span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</code></pre><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>随着功能模块的不断增加，而为了使用SQL，Hive和Streaming，需要创建单独的上下文。</p><h4 id="创建StreamingContext例子"><a href="#创建StreamingContext例子" class="headerlink" title="创建StreamingContext例子"></a>创建StreamingContext例子</h4><pre><code class="scala"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()  .setMaster(<span class="string">"local[*]"</span>)  .setAppName(<span class="string">"NetworkWordCount"</span>)  .set(<span class="string">"spark.io.compression.codec"</span>,<span class="string">"snappy"</span>)<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(args(<span class="number">2</span>).toInt))</code></pre><h2 id="SparkSession-新入口点"><a href="#SparkSession-新入口点" class="headerlink" title="SparkSession - 新入口点"></a>SparkSession - 新入口点</h2><p>众所周知，在以前的版本中，sparkcontext 是spark的入口点，因为RDD是主要的API，它是使用上下文API创建和操作的。 对于每个其他API，我们需要使用不同的context。</p><p>对于流式传输，我们需要streamingContext。 对于SQL sqlContext和hive hiveContext.，因为dataSet和DataFrame API正在成为新的独立API，我们需要为它们构建入口点。 因此在spark 2.0中，我们为DataSet和DataFrame API创建了一个新的入口点构建，称为Spark-Session。</p><img src="/Blog/2019/02/18/浅析SparkSession/145518ondfhiyihbwdoidw.jpg" title="浅析SparkSession"><h3 id="创建一个SparkSession入口"><a href="#创建一个SparkSession入口" class="headerlink" title="创建一个SparkSession入口"></a>创建一个SparkSession入口</h3><pre><code class="scala"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()  .master(<span class="string">"local"</span>)  .appName(<span class="string">"example of SparkSession"</span>)  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)  .getOrCreate()</code></pre><h3 id="完整的代码"><a href="#完整的代码" class="headerlink" title="完整的代码"></a>完整的代码</h3><pre><code class="scala"><span class="keyword">import</span> org.apache.spark.sql.{<span class="type">Row</span>, <span class="type">SparkSession</span>}<span class="keyword">import</span> org.apache.spark.sql.types._<span class="class"><span class="keyword">object</span> <span class="title">test_SparkSession</span> </span>{  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder      .appName(<span class="string">"My Spark Application"</span>)  <span class="comment">// optional and will be autogenerated if not specified</span>      .master(<span class="string">"local[*]"</span>)               <span class="comment">// avoid hardcoding the deployment environment</span>      .config(<span class="string">"spark.io.compression.codec"</span>,<span class="string">"snappy"</span>)      .getOrCreate    <span class="comment">//使用SparkSession创建DataFrame，并执行spark-SQL</span>    <span class="keyword">val</span> customSchema = <span class="type">StructType</span>(      <span class="type">List</span>(        <span class="type">StructField</span>(<span class="string">"creative"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),        <span class="type">StructField</span>(<span class="string">"uv"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)      )    )    <span class="keyword">val</span> df = spark.read.format(<span class="string">"com.databricks.spark.csv"</span>)      .schema(customSchema)                            <span class="comment">//绑定Schema</span>      .load(args(<span class="number">0</span>))      .registerTempTable(<span class="string">"creative_id_all_2"</span>)        <span class="comment">//将数据转换成零时表</span>    <span class="keyword">val</span> <span class="type">SQL</span> = spark.sql(<span class="string">"select * from creative_id_all_2 order by uv desc limit 10"</span>)      .show()    <span class="comment">//创建自定义格式</span>    <span class="keyword">val</span> schemaString = <span class="string">"name,age"</span>    <span class="keyword">val</span> fields = schemaString.split(<span class="string">","</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))    <span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)    <span class="keyword">val</span> personRDD = spark.sparkContext.textFile(args(<span class="number">0</span>))    <span class="keyword">val</span> rowRDD = personRDD.map(_.split(<span class="string">","</span>)).map(attr =&gt; <span class="type">Row</span>(attr(<span class="number">0</span>),attr(<span class="number">1</span>).trim()))    <span class="keyword">val</span> personDF = spark.createDataFrame(rowRDD,schema).show()    <span class="comment">//原有的sparkContext被包在SparkSession里面</span>    <span class="keyword">val</span> file=spark.sparkContext.textFile(args(<span class="number">2</span>))    <span class="keyword">val</span> word=file.flatMap(lines=&gt;lines.split(<span class="string">" "</span>))      .map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_)    word.foreach(println)  }}-----------------------+----------+-------+|  creative|     uv|+----------+-------+|<span class="number">1082303000</span>|<span class="number">7769653</span>||<span class="number">1126317000</span>|<span class="number">5979792</span>||<span class="number">1003303000</span>|<span class="number">5890968</span>||<span class="number">1122319000</span>|<span class="number">5616763</span>||<span class="number">1127317000</span>|<span class="number">5410128</span>||<span class="number">1124303000</span>|<span class="number">5396361</span>||<span class="number">1130317000</span>|<span class="number">5332936</span>||<span class="number">1129317000</span>|<span class="number">5293943</span>||<span class="number">1082317000</span>|<span class="number">5034792</span>||<span class="number">1086517001</span>|<span class="number">4982566</span>|+----------+-------+</code></pre><p>##拓展阅读：<br><a href="http://www.aboutyun.com/thread-25579-1-1.html" target="_blank" rel="noopener">http://www.aboutyun.com/thread-25579-1-1.html</a><br><a href="http://www.cnblogs.com/zzhangyuhang/p/9039695.html" target="_blank" rel="noopener">http://www.cnblogs.com/zzhangyuhang/p/9039695.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在理解spark-session之前让我们理解入口点，一个入口点是控制从操作系统传递到提供的程序的地方。&lt;br&gt;在2.0入口之前，spark-core是sparkContext.Apache Spark是一个功能强大的集群计算引擎，因此它专为快速计算大数据而设计。&lt;br&gt;而
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hive性能优化</title>
    <link href="http://yoursite.com/2019/02/02/Hive%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2019/02/02/Hive性能优化/</id>
    <published>2019-02-02T12:36:29.000Z</published>
    <updated>2019-03-22T07:09:47.357Z</updated>
    
    <content type="html"><![CDATA[<p>hive性能优化时，把HiveQL当做M/R程序来读，即从M/R的运行角度来考虑优化性能，从更底层思考如何优化运算性能，优化参数配置，使用Hive效率更高的函数，而不仅仅局限于逻辑代码的替换层面。</p><p>Hive性能优化</p><h2 id="性能低下的根源"><a href="#性能低下的根源" class="headerlink" title="性能低下的根源"></a>性能低下的根源</h2><h3 id="资源调度"><a href="#资源调度" class="headerlink" title="资源调度"></a>资源调度</h3><p>*根据硬件性能适当的分区，真正应用集群就像一辆机动灵活的小货车，响应快</p><ul><li>Hadoop就像吞吐量巨大的轮船，启动开销大，如果每次只做小数量的输入输出，利用率将会很低。</li><li>所以用好Hadoop的首要任务是增大每次任务所搭载的数据量。</li><li>Hadoop的核心能力是parition和sort，因而这也是优化的根本。</li><li>数据倾斜是导致效率大幅降低的主要原因，可以采用多一次 Map/Reduce 的方法，也可以使用可拆分的存储格式，建议使用后者方式。</li></ul><h3 id="运行过程"><a href="#运行过程" class="headerlink" title="运行过程"></a>运行过程</h3><ul><li>数据的大规模并不是负载重点，造成运行压力过大是因为运行数据的倾斜。</li><li>jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联对此汇总，产生几十个jobs，将会需要30分钟以上的时间且大部分时间被用于作业分配，初始化和数据输出。M/R作业初始化的时间是比较耗时间资源的一个部分。</li></ul><h3 id="函数使用"><a href="#函数使用" class="headerlink" title="函数使用"></a>函数使用</h3><ul><li>在使用SUM，COUNT，MAX，MIN等UDAF函数时，不怕数据倾斜问题，Hadoop在Map端的汇总合并优化过，使数据倾斜不成问题。</li><li>COUNT(DISTINCT)在数据量大的情况下，效率较低，如果多COUNT(DISTINCT)效率更低，因为COUNT(DISTINCT)是按GROUP BY字段分组，按DISTINCT字段排序，一般这种分布式方式是很倾斜的；比如：男UV，女UV，淘宝一天30亿的PV，如果按性别分组，分配2个reduce,每个reduce处理15亿数据。</li><li>场景允许下使用 exists / not exists 替换掉大量消耗资源的join，在RDBS中就有此优化方式，在hive早期版本中使用 left semi join 实现 exists 函数，而在新版Hive已经支持了 exists 放行使用吧。</li><li>order by 排序，只存在一个reduce，单个节点压力非常大，这样效率比较低。可以用sort by操作,通常结合distribute by使用做reduce分区键。</li></ul><h3 id="最后得出的总结"><a href="#最后得出的总结" class="headerlink" title="最后得出的总结"></a>最后得出的总结</h3><p>避实就虚，用 job 数的增加，输入量的增加，占用更多存储空间，充分利用空闲 CPU 等各种方法，分解数据倾斜造成的负担。</p><h2 id="配置角度优化"><a href="#配置角度优化" class="headerlink" title="配置角度优化"></a>配置角度优化</h2><p>我们知道了性能低下的根源，同样，我们也可以从Hive的配置解读去优化。Hive系统内部已针对不同的查询预设定了优化方法，用户可以通过调整配置进行控制， 以下举例介绍部分优化的策略以及优化控制选项。</p><h3 id="分区裁剪"><a href="#分区裁剪" class="headerlink" title="分区裁剪"></a>分区裁剪</h3><ul><li>可以在查询的过程中减少不必要的分区。</li><li>严格模式下查询必须带有分区</li></ul><p>可以减少读入的分区数目。明显提高查询速度<br>Hive 自动执行这种裁剪优化。</p><p>分区参数为：hive.optimize.pruner=true;–默认值为真</p><h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><p>写SQL要先了解数据本身的特点，如果有join ,group操作的话，要注意是否会有数据倾斜<br>如果出现数据倾斜，应当做如下处理：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.reducers.max=200;</span><br><span class="line"><span class="built_in">set</span> mapred.reduce.tasks= 200;--增大Reduce个数</span><br><span class="line"><span class="built_in">set</span> hive.groupby.mapaggr.checkinterval=100000 ;--这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置</span><br><span class="line"><span class="built_in">set</span> hive.groupby.skewindata=<span class="literal">true</span>; --如果是group by过程出现倾斜 应该设置为<span class="literal">true</span></span><br><span class="line"><span class="built_in">set</span> hive.skewjoin.key=100000; --这个是join的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置</span><br><span class="line"><span class="built_in">set</span> hive.optimize.skewjoin=<span class="literal">true</span>;--如果是join 过程出现倾斜 应该设置为<span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h3 id="小文件的合并"><a href="#小文件的合并" class="headerlink" title="小文件的合并"></a>小文件的合并</h3><p>大量的小文件导致文件数目过多，给HDFS带来压力，对hive处理的效率影响比较大，可以合并map和reduce产生的文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive.merge.mapfiles = <span class="literal">true</span>;--是否和并 Map 输出文件，默认为 True</span><br><span class="line">hive.merge.mapredfiles = <span class="literal">false</span>;--是否合并 Reduce 输出文件，默认为 False</span><br><span class="line">hive.merge.size.per.task = 256*1000*1000;--合并文件的大小</span><br></pre></td></tr></table></figure></p><h3 id="使用ORC存储"><a href="#使用ORC存储" class="headerlink" title="使用ORC存储"></a>使用ORC存储</h3><p>使用更先进的存储，如ORC，更高的压缩比，一个轻量级的索引，例如数字数据会统计min max 等，提供给优化器使用。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE vt_orc(</span><br><span class="line">  `route_id` string, </span><br><span class="line">  `mac` string, </span><br><span class="line">  `appname` string, </span><br><span class="line">  `vid` string, </span><br><span class="line">  `ts` string</span><br><span class="line">  )</span><br><span class="line">PARTITIONED BY ( </span><br><span class="line">  `date` string) </span><br><span class="line">ROW FORMAT SERDE </span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.orc.OrcSerde'</span> </span><br><span class="line">WITH SERDEPROPERTIES ( </span><br><span class="line">  <span class="string">'field.delim'</span>=<span class="string">'\u0001'</span>, </span><br><span class="line">  <span class="string">'serialization.format'</span>=<span class="string">'\u0001'</span>) </span><br><span class="line">STORED AS ORC</span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">'hdfs://ireservice/tmp/vt_orc'</span>;</span><br></pre></td></tr></table></figure></p><p>更多参考：<br><a href="http://www.cnblogs.com/xd502djj/p/3799432.html" target="_blank" rel="noopener">http://www.cnblogs.com/xd502djj/p/3799432.html</a><br><a href="https://www.cnblogs.com/smartloli/p/4356660.html" target="_blank" rel="noopener">https://www.cnblogs.com/smartloli/p/4356660.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;hive性能优化时，把HiveQL当做M/R程序来读，即从M/R的运行角度来考虑优化性能，从更底层思考如何优化运算性能，优化参数配置，使用Hive效率更高的函数，而不仅仅局限于逻辑代码的替换层面。&lt;/p&gt;
&lt;p&gt;Hive性能优化&lt;/p&gt;
&lt;h2 id=&quot;性能低下的根源&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="优化" scheme="http://yoursite.com/tags/%E4%BC%98%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【转】zookepper核心Paxos算法</title>
    <link href="http://yoursite.com/2019/01/30/%5B%E8%BD%AC%5Dzookepper%E6%A0%B8%E5%BF%83Paxos%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2019/01/30/[转]zookepper核心Paxos算法/</id>
    <published>2019-01-30T08:29:03.000Z</published>
    <updated>2019-03-22T07:06:49.823Z</updated>
    
    <content type="html"><![CDATA[<p>文章转自: <a href="https://www.cnblogs.com/endsock/p/3480093.html" target="_blank" rel="noopener">Paxos算法细节详解(一)–通过现实世界描述算法</a></p><p>最近研究paxos算法，看了许多相关的文章，概念还是很模糊，觉得还是没有掌握paxos算法的精髓，所以花了3天时间分析了libpaxos3的所有代码，此代码可以从<a href="https://bitbucket.org/sciascid/libpaxos" target="_blank" rel="noopener">https://bitbucket.org/sciascid/libpaxos</a> 下载。对paxos算法有初步了解之后，再看此文的效果会更好；如果你也想分析libpaxos3的话，此文应该会对你有不小帮助；关于paxos的历史这里不多做介绍，关于描述paxos算法写的最好的一篇文章应该就是维基百科了，地址戳这里：<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95</a></p><p>在paxos算法中，分为4种角色：<br>  Proposer ：提议者<br>  Acceptor：决策者<br>  Client：产生议题者<br>  Learner：最终决策学习者<br>上面4种角色中，提议者和决策者是很重要的，其他的2个角色在整个算法中应该算做打酱油的，Proposer就像Client的使者，由Proposer使者拿着Client的议题去向Acceptor提议，让Acceptor来决策。这里上面出现了个新名词：最终决策。现在来系统的介绍一下paxos算法中所有的行为：</p><ol><li>Proposer提出议题</li><li>Acceptor初步接受 或者 Acceptor初步不接受</li><li>如果上一步Acceptor初步接受则Proposer再次向Acceptor确认是否最终接受</li><li>Acceptor 最终接受 或者Acceptor 最终不接受<br>上面Learner最终学习的目标是Acceptor们最终接受了什么议题？注意，这里是向所有Acceptor学习，如果有多数派个Acceptor最终接受了某提议，那就得到了最终的结果，算法的目的就达到了。画一幅图来更加直观：</li></ol><p>为什么需要3个Acceptor？因为Acceptor必须是最少大于等于3个，并且必须是奇数个，因为要形成多数派嘛，如果是偶数个，比如4个，2个接受2个不接受，各执己见，没法搞下去了。<br>为什么是3个Proposer？ 其实无所谓是多少个了，1~n 都可以的；如果是1个proposer，毫无竞争压力，很顺利的完成2阶段提交，Acceptor们最终批准了事。如果是多个proposer就比较复杂了，请继续看。</p><p>上面的图中是画了很多节点的，每个节点需要一台机器么？答案是不需要的，上面的图是逻辑图，物理中，可以将Acceptor和Proposer以及Client放到一台机器上，只是使用了不同的端口号罢了，Acceptor们启动不同端口的TCP监听，Proposer来主动连接即可；完全可以将Client、Proposer、Acceptor、Learner合并到一个程序里面；这里举一个例子：比如开发一个JOB程序，JOB程序部署在多台服务器上(数量为奇数)，这些JOB有可能同时处理一项任务，现在使用paxos算法让这些JOB自己来商量由谁(哪台机器)来处理这项任务，这样JOB程序里就需要包含Client、Proposer、Acceptor、Learner这4大功能，并且需要配置其他JOB服务器的IP地址。<br>再举一个例子，zookeeper常常用来做分布式事务锁。Zookeeper所使用的zad协议也是类似paxos协议的。所有分布式自协商一致性算法都是paxos算法的简化或者变种。Client是使用zookeeper服务的机器，Zookeeper自身包含了Acceptor, Proposer, Learner。Zookeeper领导选举就是paxos过程，还有Client对Zookeeper写Znode时，也是要进行Paxos过程的，因为不同Client可能连接不同的Zookeeper服务器来写Znode，到底哪个Client才能写成功？需要依靠Zookeeper的paxos保证一致性，写成功Znode的Client自然就是被最终接受了，Znode包含了写入Client的IP与端口，其他的Client也可以读取到这个Znode来进行Learner。也就是说在Zookeeper自身包含了Learner(因为Zookeeper为了保证自身的一致性而会进行领导选举，所以需要有Learner的内部机制，多个Zookeeper服务器之间需要知道现在谁是领导了)，Client端也可以Learner，Learner是广义的。</p><p>现在通过一则故事来学习paxos的算法的流程(2阶段提交)，有2个Client(老板，老板之间是竞争关系)和3个Acceptor(政府官员)：</p><ol><li>现在需要对一项议题来进行paxos过程，议题是“A项目我要中标！”，这里的“我”指每个带着他的秘书Proposer的Client老板。</li><li>Proposer当然听老板的话了，赶紧带着议题和现金去找Acceptor政府官员。</li><li>作为政府官员，当然想谁给的钱多就把项目给谁。</li><li>Proposer-1小姐带着现金同时找到了Acceptor-1~Acceptor-3官员，1与2号官员分别收取了10比特币，找到第3号官员时，没想到遭到了3号官员的鄙视，3号官员告诉她，Proposer-2给了11比特币。不过没关系，Proposer-1已经得到了1,2两个官员的认可，形成了多数派(如果没有形成多数派，Proposer-1会去银行提款在来找官员们给每人20比特币，这个过程一直重复每次+10比特币，直到多数派的形成)，满意的找老板复命去了，但是此时Proposer-2保镖找到了1,2号官员，分别给了他们11比特币，1,2号官员的态度立刻转变，都说Proposer-2的老板懂事，这下子Proposer-2放心了，搞定了3个官员，找老板复命去了，当然这个过程是第一阶段提交，只是官员们初步接受贿赂而已。故事中的比特币是编号，议题是value。<br>这个过程保证了在某一时刻，某一个proposer的议题会形成一个多数派进行初步支持；<br>===============华丽的分割线，第一阶段结束================<br>5.　现在进入第二阶段提交，现在proposer-1小姐使用分身术(多线程并发)分了3个自己分别去找3位官员，最先找到了1号官员签合同，遭到了1号官员的鄙视，1号官员告诉他proposer-2先生给了他11比特币，因为上一条规则的性质proposer-1小姐知道proposer-2第一阶段在她之后又形成了多数派(至少有2位官员的赃款被更新了);此时她赶紧去提款准备重新贿赂这3个官员(重新进入第一阶段)，每人20比特币。刚给1号官员20比特币， 1号官员很高兴初步接受了议题，还没来得及见到2,3号官员的时候<br>这时proposer-2先生也使用分身术分别找3位官员(注意这里是proposer-2的第二阶段)，被第1号官员拒绝了告诉他收到了20比特币，第2,3号官员顺利签了合同，这时2，3号官员记录client-2老板用了11比特币中标，因为形成了多数派，所以最终接受了Client2老板中标这个议题，对于proposer-2先生已经出色的完成了工作；<br>这时proposer-1小姐找到了2号官员，官员告诉她合同已经签了，将合同给她看，proposer-1小姐是一个没有什么职业操守的聪明人，觉得跟Client1老板混没什么前途，所以将自己的议题修改为“Client2老板中标”，并且给了2号官员20比特币，这样形成了一个多数派。顺利的再次进入第二阶段。由于此时没有人竞争了，顺利的找3位官员签合同，3位官员看到议题与上次一次的合同是一致的，所以最终接受了，形成了多数派，proposer-1小姐跳槽到Client2老板的公司去了。<br>===============华丽的分割线，第二阶段结束===============<br>Paxos过程结束了，这样，一致性得到了保证，算法运行到最后所有的proposer都投“client2中标”所有的acceptor都接受这个议题，也就是说在最初的第二阶段，议题是先入为主的，谁先占了先机，后面的proposer在第一阶段就会学习到这个议题而修改自己本身的议题，因为这样没职业操守，才能让一致性得到保证，这就是paxos算法的一个过程。原来paxos算法里的角色都是这样的不靠谱，不过没关系，结果靠谱就可以了。该算法就是为了追求结果的一致性。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;文章转自: &lt;a href=&quot;https://www.cnblogs.com/endsock/p/3480093.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paxos算法细节详解(一)–通过现实世界描述算法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最近研究pa
      
    
    </summary>
    
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【转】一致性Hash</title>
    <link href="http://yoursite.com/2019/01/24/%5B%E8%BD%AC%5D%E4%B8%80%E8%87%B4%E6%80%A7Hash/"/>
    <id>http://yoursite.com/2019/01/24/[转]一致性Hash/</id>
    <published>2019-01-24T11:26:09.000Z</published>
    <updated>2019-03-22T07:03:22.261Z</updated>
    
    <content type="html"><![CDATA[<p>文章转自: <a href="https://www.cnblogs.com/coder2012/p/3973877.html" target="_blank" rel="noopener">一致性Hash</a></p><p>一致性Hash<br>一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题，经常用于分布式、负载均衡等。<br>原理<br>一致哈希是一种特殊的哈希算法。在使用一致哈希算法后，哈希表中平均只需要对</p><p> 个关键字重新映射，其中 </p><p>是关键字的数量，</p><p>是映射节点数量。然而在传统的哈希表中，添加或删除一个映射节点的几乎需要对所有关键字进行重新映射。<br>原来的映射大概是这样的，如下图，没当加入或删除一个新的节点可能都会造成每个节点的映射发生变化，如果黄色的节点代表服务器，那么每一次更新服务器的数量都会造成每个服务器上蓝色的映射节点都会发生变化，当集群数量庞大时每次增删节点所需要的修改操作就会过于庞大。</p><img src="/Blog/2019/01/24/[转]一致性Hash/172209161901412.png" title="[转]一致性Hash"><p>而在一致性哈希中映射是这样的，如下图，一般一致性hash取值范围为-2^32~2^32，分布在一个圆上</p><img src="/Blog/2019/01/24/[转]一致性Hash/172236518155109.gif" title="[转]一致性Hash"><p>下面画的比较丑，就凑合看吧~~<br>黄色节点作为映射节点（实节点），蓝色节点为需要映射到映射节点的key节点<br>    • 首先，看左边的图，把8个蓝色的key通过hash取值散列在一个范围为0~2^32的圆上<br>    • 其次，选择三个黄色节点作为映射节点，按照圆的顺时针方向，把蓝色节点与黄色节点建立映射关系<br>    • 最后，由于1节点负载为4，最大，那么我们为了降低1节点的负载情况，增加黄色的映射节点4，依然按照顺时针的方向修改原映射，那么只需要改变蓝色的节点7、8以及黄色节点1</p><img src="/Blog/2019/01/24/[转]一致性Hash/172234080342046.png" title="[转]一致性Hash"><p>实现<br>一般为了方便起见，我们把黄色的映射节点称为实节点，也就是固定不变的，而蓝色的节点称为虚节点，虚节点需要映射到实节点，每次实节点的增删只会影响距离它最近的节点。<br>在这里使用C++实现了ConsistentHash算法</p><img src="/Blog/2019/01/24/[转]一致性Hash/172301305039793.png" title="[转]一致性Hash"><p>在存储节点方面，本程序只是简单的使用链表，最好的方式当然是红黑树了，当然为了简单起见，就用了链表，主要是理解一致性hash的原理~~</p><p>源码下载：<a href="https://github.com/yxd123/algorithm-notes/tree/master/ConsistentHash" target="_blank" rel="noopener">https://github.com/yxd123/algorithm-notes/tree/master/ConsistentHash</a><br>参考<br><a href="http://blog.csdn.net/cywosp/article/details/23397179" target="_blank" rel="noopener">http://blog.csdn.net/cywosp/article/details/23397179</a><br><a href="http://zh.wikipedia.org/wiki/一致哈希" target="_blank" rel="noopener">http://zh.wikipedia.org/wiki/一致哈希</a><br><a href="http://baike.baidu.com/view/1588037.htm?fr=aladdin" target="_blank" rel="noopener">http://baike.baidu.com/view/1588037.htm?fr=aladdin</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;文章转自: &lt;a href=&quot;https://www.cnblogs.com/coder2012/p/3973877.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;一致性Hash&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一致性Hash&lt;br&gt;一致性哈希算法在199
      
    
    </summary>
    
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>浅析HDFS原理</title>
    <link href="http://yoursite.com/2019/01/18/%E6%B5%85%E6%9E%90HDFS%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/01/18/浅析HDFS原理/</id>
    <published>2019-01-18T10:32:34.000Z</published>
    <updated>2019-03-22T07:01:19.633Z</updated>
    
    <content type="html"><![CDATA[<p>文章转自: <a href="https://www.cnblogs.com/laov/p/3434917.html" target="_blank" rel="noopener">HDFS的运行原理</a></p><p>HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>HDFS有很多特点：</p><ul><li>保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。</li><li>运行在廉价的机器上。</li><li>适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。</li></ul><h2 id="功能模块"><a href="#功能模块" class="headerlink" title="功能模块"></a>功能模块</h2><img src="/Blog/2019/01/18/浅析HDFS原理/26104230-8109ac513de14fe1b115b775581751f2.jpg" title="浅析HDFS原理"><p>HDFS也是按照Master和Slave的结构。分NameNode、SecondaryNameNode、DataNode这几个角色。<br>NameNode：是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；<br>SecondaryNameNode：是一个小弟，分担大哥namenode的工作量；是NameNode的冷备份；合并fsimage和fsedits然后再发给namenode。<br>DataNode：Slave节点，奴隶，干活的。负责存储client发来的数据块block；执行数据块的读写操作。<br>热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。<br>冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。<br>fsimage:元数据镜像文件（文件系统的目录树。）<br>edits：元数据的操作日志（针对文件系统做的修改操作记录）<br>namenode内存中存储的是=fsimage+edits。<br>SecondaryNameNode负责定时默认1小时，从namenode上，获取fsimage和edits来进行合并，然后再发送给namenode。减少namenode的工作量。</p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h3><img src="/Blog/2019/01/18/浅析HDFS原理/26162921-2de9d28df9b54fe6a97a6fd88f1cb03f.jpg" title="浅析HDFS原理"><p>有一个文件FileA，100M大小。Client将FileA写入到HDFS上。<br>HDFS按默认配置。<br>HDFS分布在三个机架上Rack1，Rack2，Rack3。</p><p>a. Client将FileA按64M分块。分成两块，block1和Block2;<br>b. Client向nameNode发送写数据请求，如图蓝色虚线①——&gt;。<br>c. NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———&gt;。<br>    Block1: host2,host1,host3<br>    Block2: host7,host8,host4<br>    原理：NameNode具有RackAware机架感知功能，这个可以配置。<br>    若client为DataNode节点，那存储block时，规则为：副本1，同client的节点上；副本2，不同机架节点上；副本3，同第二个副本机架的另一个节点上；其他副本随机挑选。<br>    若client不为DataNode节点，那存储block时，规则为：副本1，随机选择一个节点上；副本2，不同副本1，机架上；副本3，同副本2相同的另一个节点上；其他副本随机挑选。<br>d. client向DataNode发送block1；发送过程是以流式写入。</p><h3 id="流式写入过程，"><a href="#流式写入过程，" class="headerlink" title="流式写入过程，"></a>流式写入过程，</h3><pre><code>1&gt;将64M的block1按64k的package划分;2&gt;然后将第一个package发送给host2;3&gt;host2接收完后，将第一个package发送给host1，同时client想host2发送第二个package；4&gt;host1接收完第一个package后，发送给host3，同时接收host2发来的第二个package。5&gt;以此类推，如图红线实线所示，直到将block1发送完毕。6&gt;host2,host1,host3向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。7&gt;client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就真完成了。如图黄色粗实线8&gt;发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。9&gt;发送完block2后，host7,host8,host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。10&gt;client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。</code></pre><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>通过写过程，我们可以了解到：</p><ul><li>写1T文件，我们需要3T的存储，3T的网络流量贷款。</li><li>在执行读或写的过程中，NameNode和DataNode通过HeartBeat进行保存通信，确定DataNode活着。如果发现DataNode死掉了，就将死掉的DataNode上的数据，放到其他节点去。读取时，要读其他节点去。</li><li>挂掉一个节点，没关系，还有其他节点可以备份；甚至，挂掉某一个机架，也没关系；其他机架上，也有备份。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;文章转自: &lt;a href=&quot;https://www.cnblogs.com/laov/p/3434917.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HDFS的运行原理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;HDFS（Hadoop Distributed F
      
    
    </summary>
    
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Spark安装指南</title>
    <link href="http://yoursite.com/2018/12/11/spark%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/"/>
    <id>http://yoursite.com/2018/12/11/spark安装手册/</id>
    <published>2018-12-11T03:34:48.742Z</published>
    <updated>2019-03-21T03:38:05.080Z</updated>
    
    <content type="html"><![CDATA[<p>在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。<br>一步步你会成功！</p><h2 id="1-Java运行环境"><a href="#1-Java运行环境" class="headerlink" title="1.Java运行环境"></a>1.Java运行环境</h2><h3 id="下载JDK"><a href="#下载JDK" class="headerlink" title="下载JDK"></a>下载JDK</h3><pre><code class="bash">wget --no-cookies --no-check-certificate --header <span class="string">"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie"</span> <span class="string">"http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz"</span>tar -zxvf jdk-8u171-linux-x64.tar.gzmv jdk1.8.0_171/ /usr/<span class="built_in">local</span>/</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/profile--添加以下<span class="built_in">set</span> java environmentexportJAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171exportJRE_HOME=<span class="variable">${JAVA_HOME}</span>/jreexportCLASSPATH=.:<span class="variable">${JAVA_HOME}</span>/lib/dt.JAVA_HOME/lib/tools.jar:<span class="variable">${JRE_HOME}</span>/libexportPATH=<span class="variable">${JAVA_HOME}</span>/bin:<span class="variable">${PATH}</span></code></pre><p>使用命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/profile</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/environmentPATH=<span class="string">"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:<span class="variable">$JAVA_HOME</span>/bin"</span><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><p>输入以下命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/environment</code></pre><h2 id="2-hadoop"><a href="#2-hadoop" class="headerlink" title="2.hadoop"></a>2.hadoop</h2><h3 id="hadoop版本下载"><a href="#hadoop版本下载" class="headerlink" title="hadoop版本下载"></a>hadoop版本下载</h3><p>hadoop-2.7.6.tar.gz</p><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz"</span>``` bash``` bashtar -zxvf hadoop-2.7.6.tar.gz -C /usr/</code></pre><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> HADOOP_HOME=/usr/hadoop-2.7.6<span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbinvim /usr/hadoop-2.7.6/etc/hadoop/hadoop-env.sh<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><h3 id="配置文件设置"><a href="#配置文件设置" class="headerlink" title="配置文件设置"></a>配置文件设置</h3><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/hadoop/tmp&lt;/value&gt;&lt;/property&gt;vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;  &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/hdfs-site.xml&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/datanode&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="hadoop初始化和启动"><a href="#hadoop初始化和启动" class="headerlink" title="hadoop初始化和启动"></a>hadoop初始化和启动</h3><p>初始化hadoop：</p><pre><code class="bash">/usr/hadoop-2.7.6/bin/hdfs namenode -format</code></pre><p>##启动hadoop</p><pre><code class="bash"><span class="variable">$HADOOP_HOME</span>/sbin/start-all.sh</code></pre><h2 id="scala安装"><a href="#scala安装" class="headerlink" title="scala安装"></a>scala安装</h2><pre><code class="bash">wget <span class="string">"https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz"</span>tar -zxvf scala-2.11.12.tgzmv scala-2.11.12/ /usr/<span class="built_in">local</span>/vim /etc/profile<span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:/usr/local/scala-2.11.12/bin"</span></code></pre><h2 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h2><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz"</span>tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C /usr/</code></pre><p>###添加环境变量</p><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7<span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin:<span class="variable">$PATH</span></code></pre><p>###修改spark配置</p><pre><code class="bash">cp /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh.template /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.shvim /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh<span class="built_in">export</span> SPARK_MASTER_IP=localhost<span class="built_in">export</span> SPARK_WORKER_MEMORY=8g <span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171<span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala-2.11.12 <span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7 <span class="built_in">export</span> HADOOP_CONF_DIR=/usr/hadoop-2.7.6/etc/hadoop<span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_HOME</span>/lib <span class="built_in">export</span> SCALA_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span> <span class="built_in">export</span> SPARK_WORKER_CORES=5<span class="built_in">export</span> SPARK_WORKER_INSTANCES=1<span class="built_in">export</span> SPARK_MASTER_PORT=7077</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。&lt;br&gt;一步步你会成功！&lt;/p&gt;
&lt;h2 id=&quot;1-Java运行环境&quot;&gt;&lt;a href=&quot;#1-Java运行环境&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/12/11/hello-world/"/>
    <id>http://yoursite.com/2018/12/11/hello-world/</id>
    <published>2018-12-11T02:52:31.590Z</published>
    <updated>2018-12-09T09:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
