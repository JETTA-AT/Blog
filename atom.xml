<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jet Smith</title>
  
  
  <link href="/Blog/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-21T09:11:22.074Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jet Smith</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅析ACID、BASE、CAP原理</title>
    <link href="http://yoursite.com/2019/03/03/%E6%B5%85%E6%9E%90ACID%E3%80%81BASE%E3%80%81CAP%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/03/03/浅析ACID、BASE、CAP原理/</id>
    <published>2019-03-03T08:56:37.000Z</published>
    <updated>2019-03-21T09:11:22.074Z</updated>
    
    <content type="html"><![CDATA[<img src="/Blog/2019/03/03/浅析ACID、BASE、CAP原理/20190321170312.png" title="浅析ACID、BASE、CAP原理"><h3 id="浅析ACID、BASE、CAP原理"><a href="#浅析ACID、BASE、CAP原理" class="headerlink" title="浅析ACID、BASE、CAP原理"></a>浅析ACID、BASE、CAP原理</h3><ul><li>ACID保证永久更新数据库<ul><li>原子性Atomic<ul><li>不可分割，只有全部成功，才算成功</li></ul></li><li>一致性Consistency<ul><li>不能破坏业务逻辑上的一致性，转账存款总额不变</li><li>数据库的约束 级联和Trigger都必须满足事务的一致性</li></ul></li><li>隔离性Isolation<ul><li>并发环境中，各事务都有各自完整数据空间<ul><li>事务锁</li></ul></li></ul></li><li>持久性Durability<ul><li>事务结束，数据库更改必须永久保存，重启依旧恢复事务成功结束状态</li></ul></li></ul></li><li>CAP理论<ul><li>一致性Consistency<ul><li>同样数据在分布式系统中所有地方都是被复制成相同</li></ul></li><li>可用性Availability<ul><li>所有在分布式系统活跃的节点都能处理操作且能响应查询</li><li>可用并不意味着数据的一致，比如过期的数据或脏读</li></ul></li><li>分区容错性Partition tolerancce<ul><li>除整个网络故障外，均导致整个系统无法正确响应</li></ul></li><li>理论：一个分布式系统最多只能同时满足三项中的两项</li></ul></li><li>BASE理论<ul><li>核心思想<ul><li>基于CAP理论何其基础上的延伸出来的 BASE 理论，有人提出柔性事务概念</li><li>即时无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性</li><li>与 ACID 相比，以牺牲强一致性获得高可用性</li></ul></li><li>基本可用 Basically Available<ul><li>核心可用，系统故障允许损失大部分可用性</li><li>降级服务，应对访问量激增，屏蔽一些功能</li></ul></li><li>柔性状态 Soft State<ul><li>允许中间状态，并不影响整体可用性<ul><li>例如三个副本，允许不同节点件的副本同步延迟时</li></ul></li></ul></li><li>最终一致性 Eventual Consitency<ul><li>副本经过一定时间后，最终达到一致状态</li></ul></li></ul></li><li>后记<ul><li>单机系统总线不会丢数据，而网络会。机器间的通讯，可能是收到，未收到，不知道收没收到，同步状态成本很高<ul><li>Paxos 确保一致性，但只有到一半以上确认后才能确认成功，这种强同步不是最终一致性</li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;/Blog/2019/03/03/浅析ACID、BASE、CAP原理/20190321170312.png&quot; title=&quot;浅析ACID、BASE、CAP原理&quot;&gt;
&lt;h3 id=&quot;浅析ACID、BASE、CAP原理&quot;&gt;&lt;a href=&quot;#浅析ACID、BASE
      
    
    </summary>
    
    
      <category term="原理" scheme="http://yoursite.com/tags/%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>《企业IT架构转型之道》小结</title>
    <link href="http://yoursite.com/2019/02/27/%E3%80%8A%E4%BC%81%E4%B8%9AIT%E6%9E%B6%E6%9E%84%E8%BD%AC%E5%9E%8B%E4%B9%8B%E9%81%93%E3%80%8B%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2019/02/27/《企业IT架构转型之道》小结/</id>
    <published>2019-02-27T08:56:37.000Z</published>
    <updated>2019-03-21T09:16:35.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="企业IT架构转型之道"><a href="#企业IT架构转型之道" class="headerlink" title="企业IT架构转型之道"></a>企业IT架构转型之道</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="大数据平台"><a href="#大数据平台" class="headerlink" title="大数据平台"></a>大数据平台</h3><ul><li>数据层访问打通，数据权限的控制。</li><li>数据格式的转换，数据清洗，数据同步。<h3 id="人的配置"><a href="#人的配置" class="headerlink" title="人的配置"></a>人的配置</h3></li><li>缺少能基于数据有业务建模能力的专家</li><li>数据采集，数学算法，数学软件，数据分析，预测分析，市场应用，决策分析<h3 id="人的发展"><a href="#人的发展" class="headerlink" title="人的发展"></a>人的发展</h3></li><li>烟囱式系统建设，不同的角色技术人员很难对某一业务领域有持续的理解和沉淀。</li><li>而采用环绕服务能力持续运营构建独立组织的形态，懂技术懂业务的复合型人才<h2 id="共享服务架构选择-amp-建设原则"><a href="#共享服务架构选择-amp-建设原则" class="headerlink" title="共享服务架构选择&amp;建设原则"></a>共享服务架构选择&amp;建设原则</h2><h3 id="企业服务总线（ESB）-实现SOA方案"><a href="#企业服务总线（ESB）-实现SOA方案" class="headerlink" title="企业服务总线（ESB） 实现SOA方案"></a>企业服务总线（ESB） 实现SOA方案</h3></li><li>HSF<ul><li>每一个HSF应用均是以War包形式存在，运行在Tomcat容器，Tomcat容器层集成HSF服务框架，无需引入Jar</li><li>Nginx提供服务器和Diamond的列表信息</li><li>HSF框架采用Netty+Hession 数据序列化协议实现服务交互</li><li>这类RPC协议采用多路复用的TCP长连接，多个服务请求调用同一个长连接</li><li>TPS上达到10w性能远比 REST 或 Web Service 高<h3 id="共享服务中心"><a href="#共享服务中心" class="headerlink" title="共享服务中心"></a>共享服务中心</h3></li></ul></li><li>设计</li><li>运营</li><li>工程</li><li>概要: 原则<ul><li>高内聚、低耦合<ul><li>业务之间依赖性很大，但服务间隔离性很大</li></ul></li><li>数据完整性</li><li>业务可运营性原则（巡检）<ul><li>业务逻辑，数据沉淀，产生价值</li></ul></li><li>渐进性的建设原则<h2 id="数据库能力线性扩展"><a href="#数据库能力线性扩展" class="headerlink" title="数据库能力线性扩展"></a>数据库能力线性扩展</h2><h3 id="跨库查询"><a href="#跨库查询" class="headerlink" title="跨库查询"></a>跨库查询</h3></li></ul></li><li>分布式数据层框架TDDL（数据中间件）<ul><li>Matrix层（TDataSourcre）实现分库分表逻辑，底下持有多个GroupDs实例</li><li>Group层（TGroupSource）实现数据库的主备/读写分离逻辑，底层持有多个AtomDs实例</li><li>Atom层（TAtomDataSource）实现数据库连接等信息的动态推送，持有原子的数据源<h2 id="异步化与缓存原则"><a href="#异步化与缓存原则" class="headerlink" title="异步化与缓存原则"></a>异步化与缓存原则</h2><h3 id="数据库事务异步化（将大事务拆成小事务，降低锁占用）"><a href="#数据库事务异步化（将大事务拆成小事务，降低锁占用）" class="headerlink" title="数据库事务异步化（将大事务拆成小事务，降低锁占用）"></a>数据库事务异步化（将大事务拆成小事务，降低锁占用）</h3></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;企业IT架构转型之道&quot;&gt;&lt;a href=&quot;#企业IT架构转型之道&quot; class=&quot;headerlink&quot; title=&quot;企业IT架构转型之道&quot;&gt;&lt;/a&gt;企业IT架构转型之道&lt;/h1&gt;&lt;h2 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="架构" scheme="http://yoursite.com/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Spark函数:浅析combineByKey、reduceByKey、groupByKey</title>
    <link href="http://yoursite.com/2019/02/21/%E8%AE%A4%E7%9F%A5combineByKey%E3%80%81reduceByKey%E3%80%81groupByKey/"/>
    <id>http://yoursite.com/2019/02/21/认知combineByKey、reduceByKey、groupByKey/</id>
    <published>2019-02-21T08:56:37.000Z</published>
    <updated>2019-03-21T08:40:01.469Z</updated>
    
    <content type="html"><![CDATA[<p>简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。</p><h2 id="combineByKey函数"><a href="#combineByKey函数" class="headerlink" title="combineByKey函数"></a>combineByKey函数</h2><pre><code class="bash">def combineByKey[C](        createCombiner: V =&gt; C,        mergeValue: (C, V) =&gt; C,        mergeCombiners: (C, C) =&gt; C,        partitioner: Partitioner,        mapSideCombine: Boolean = <span class="literal">true</span>,        serializer: Serializer = null)</code></pre><h3 id="三个自定义方法"><a href="#三个自定义方法" class="headerlink" title="三个自定义方法"></a>三个自定义方法</h3><h4 id="1-createCombiner"><a href="#1-createCombiner" class="headerlink" title="1.createCombiner"></a>1.createCombiner</h4><p>这个函数把当前rdd中的值（value）作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作，分区内操作)</p><h4 id="2-mergeValue"><a href="#2-mergeValue" class="headerlink" title="2. mergeValue"></a>2. mergeValue</h4><p>该函数把元素V合并到之前的元素C(createCombiner)上 (每个分区内合并)</p><h4 id="3-mergeCombiner"><a href="#3-mergeCombiner" class="headerlink" title="3. mergeCombiner"></a>3. mergeCombiner</h4><p>该函数把2个元素C合并 (此函数作用范围在rdd的不同分区间内，跨分区合并)</p><h3 id="使用combineByKey求平均数"><a href="#使用combineByKey求平均数" class="headerlink" title="使用combineByKey求平均数"></a>使用combineByKey求平均数</h3><p>测试代码如下：</p><pre><code class="bash">import org.apache.spark.{SparkConf, SparkContext}object combineByKey {  def main(args: Array[String]): Unit = {    val sparkConf = new SparkConf()      .<span class="built_in">set</span>(<span class="string">"spark.io.compression.codec"</span>, <span class="string">"snappy"</span>)      .setAppName(<span class="string">"dsp_get_request"</span>).setMaster(<span class="string">"local[*]"</span>)    val sc = new SparkContext(sparkConf)    val initialScores = Array((<span class="string">"Fred"</span>, 88.0), (<span class="string">"Fred"</span>, 95.0), (<span class="string">"Fred"</span>, 91.0), (<span class="string">"Wilma"</span>, 93.0), (<span class="string">"Wilma"</span>, 95.0), (<span class="string">"Wilma"</span>, 98.0))    val d1 = sc.parallelize(initialScores)    <span class="built_in">type</span> MVType = (Int, Double) //定义一个元组类型(科目计数器,分数)    val rdd2 = d1.combineByKey(      score =&gt; (1,score),  // 将score映射为一个元组，作为分区内聚合初始值      (c1: MVType , newscore) =&gt; (c1._1 + 1 , c1._2 + newscore),  //分区内聚合      (c1: MVType ,c2: MVType) =&gt; (c1._1 + c2._1,c1._2 + c2._2)  //分区间聚合    ).map{ <span class="keyword">case</span>(name ,(num ,cnt)) =&gt; (name ,num / cnt)}    rdd2.foreach(println)  }}--------------------------------------------------(Fred,91.33333333333333)(Wilma,95.33333333333333)</code></pre><h4 id="使用reduceByKey求平均数"><a href="#使用reduceByKey求平均数" class="headerlink" title="使用reduceByKey求平均数"></a>使用reduceByKey求平均数</h4><p>现在用reduceByKey改写刚才的平均数算法</p><pre><code class="bash">val rdd3 = d1.map(a =&gt; (a._1, (a._2, 1)))  .reduceByKey((a,b) =&gt; (a._1+b._1,a._2+b._2))  .map(t =&gt; (t._1,t._2._1/t._2._2))rdd3.foreach(println)</code></pre><h2 id="reduceByKey、groupByKey"><a href="#reduceByKey、groupByKey" class="headerlink" title="reduceByKey、groupByKey"></a>reduceByKey、groupByKey</h2><p>我们再看看reduceByKey、groupByKey的区别：</p><pre><code class="bash">d1.foreach(println)------------------------(Wilma,93.0)(Fred,95.0)(Fred,88.0)(Fred,91.0)(Wilma,95.0)(Wilma,98.0)d1.reduceByKey(_ + _).foreach(println)-------------------------(Wilma,286.0)(Fred,274.0)d1.groupByKey().foreach(println)-------------------------(Wilma,CompactBuffer(93.0, 95.0, 98.0))(Fred,CompactBuffer(88.0, 95.0, 91.0))</code></pre><h3 id="reduceByKey效率会更高"><a href="#reduceByKey效率会更高" class="headerlink" title="reduceByKey效率会更高"></a>reduceByKey效率会更高</h3><p>其中reduceByKey(_ + _)写法同等于reduceByKey((a,b) =&gt; (a + b))<br>groupByKey会把所有的键值对集合都加载到内存中存储计算，若一个键值对太多，则会导致内存溢出。<br>reduceByKey 进行shuffle之前会在map做合并，这样就会减少shuffle 的IO 传送，效率会高一些。</p><h3 id="groupByKey聚合"><a href="#groupByKey聚合" class="headerlink" title="groupByKey聚合"></a>groupByKey聚合</h3><pre><code class="bash">d1.groupByKey().map(t =&gt; (t._1,t._2.sum)).foreach(println)-------------------------(Fred,274.0)(Wilma,286.0)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。&lt;/p&gt;
&lt;h2 id=&quot;combineB
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark函数:浅析SparkSession</title>
    <link href="http://yoursite.com/2019/02/18/%E6%B5%85%E6%9E%90SparkSession/"/>
    <id>http://yoursite.com/2019/02/18/浅析SparkSession/</id>
    <published>2019-02-18T09:21:37.000Z</published>
    <updated>2019-03-21T08:44:29.689Z</updated>
    
    <content type="html"><![CDATA[<p>在理解spark-session之前让我们理解入口点，一个入口点是控制从操作系统传递到提供的程序的地方。<br>在2.0入口之前，spark-core是sparkContext.Apache Spark是一个功能强大的集群计算引擎，因此它专为快速计算大数据而设计。<br>而这个入口可就是SparkContext。</p><h2 id="SparkContext在Apache-Spark中的功能："><a href="#SparkContext在Apache-Spark中的功能：" class="headerlink" title="SparkContext在Apache Spark中的功能："></a>SparkContext在Apache Spark中的功能：</h2><p>1.获取spark应用程序的当前状态<br>2.设置配置<br>3.访问各种服务<br>4.取消job<br>5.取消一个stage<br>6.关闭清洁<br>7.注册Spark-Listener<br>8.可编程动态分配<br>9.访问持久性RDD</p><p>SparkConf是创建spark上下文对象所必需的，它存储配置参数，如appName（用于标识spark驱动程序），core的数目和在工作节点上运行的执行程序的内存大小。</p><pre><code class="scala"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()    <span class="comment">//core数目 local 本地单核/ local[2] 本地2核 / local[*]  本地全部核心</span>  .setMaster(<span class="string">"local"</span>)    .setAppName(<span class="string">"Spark Practice"</span>)<span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</code></pre><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>随着功能模块的不断增加，而为了使用SQL，Hive和Streaming，需要创建单独的上下文。</p><h4 id="创建StreamingContext例子"><a href="#创建StreamingContext例子" class="headerlink" title="创建StreamingContext例子"></a>创建StreamingContext例子</h4><pre><code class="scala"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()  .setMaster(<span class="string">"local[*]"</span>)  .setAppName(<span class="string">"NetworkWordCount"</span>)  .set(<span class="string">"spark.io.compression.codec"</span>,<span class="string">"snappy"</span>)<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(args(<span class="number">2</span>).toInt))</code></pre><h2 id="SparkSession-新入口点"><a href="#SparkSession-新入口点" class="headerlink" title="SparkSession - 新入口点"></a>SparkSession - 新入口点</h2><p>众所周知，在以前的版本中，sparkcontext 是spark的入口点，因为RDD是主要的API，它是使用上下文API创建和操作的。 对于每个其他API，我们需要使用不同的context。</p><p>对于流式传输，我们需要streamingContext。 对于SQL sqlContext和hive hiveContext.，因为dataSet和DataFrame API正在成为新的独立API，我们需要为它们构建入口点。 因此在spark 2.0中，我们为DataSet和DataFrame API创建了一个新的入口点构建，称为Spark-Session。</p><img src="/Blog/2019/02/18/浅析SparkSession/145518ondfhiyihbwdoidw.jpg" title="浅析SparkSession"><h3 id="创建一个SparkSession入口"><a href="#创建一个SparkSession入口" class="headerlink" title="创建一个SparkSession入口"></a>创建一个SparkSession入口</h3><pre><code class="scala"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()  .master(<span class="string">"local"</span>)  .appName(<span class="string">"example of SparkSession"</span>)  .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)  .getOrCreate()</code></pre><h3 id="完整的代码"><a href="#完整的代码" class="headerlink" title="完整的代码"></a>完整的代码</h3><pre><code class="scala"><span class="keyword">import</span> org.apache.spark.sql.{<span class="type">Row</span>, <span class="type">SparkSession</span>}<span class="keyword">import</span> org.apache.spark.sql.types._<span class="class"><span class="keyword">object</span> <span class="title">test_SparkSession</span> </span>{  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder      .appName(<span class="string">"My Spark Application"</span>)  <span class="comment">// optional and will be autogenerated if not specified</span>      .master(<span class="string">"local[*]"</span>)               <span class="comment">// avoid hardcoding the deployment environment</span>      .config(<span class="string">"spark.io.compression.codec"</span>,<span class="string">"snappy"</span>)      .getOrCreate    <span class="comment">//使用SparkSession创建DataFrame，并执行spark-SQL</span>    <span class="keyword">val</span> customSchema = <span class="type">StructType</span>(      <span class="type">List</span>(        <span class="type">StructField</span>(<span class="string">"creative"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),        <span class="type">StructField</span>(<span class="string">"uv"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)      )    )    <span class="keyword">val</span> df = spark.read.format(<span class="string">"com.databricks.spark.csv"</span>)      .schema(customSchema)                            <span class="comment">//绑定Schema</span>      .load(args(<span class="number">0</span>))      .registerTempTable(<span class="string">"creative_id_all_2"</span>)        <span class="comment">//将数据转换成零时表</span>    <span class="keyword">val</span> <span class="type">SQL</span> = spark.sql(<span class="string">"select * from creative_id_all_2 order by uv desc limit 10"</span>)      .show()    <span class="comment">//创建自定义格式</span>    <span class="keyword">val</span> schemaString = <span class="string">"name,age"</span>    <span class="keyword">val</span> fields = schemaString.split(<span class="string">","</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))    <span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)    <span class="keyword">val</span> personRDD = spark.sparkContext.textFile(args(<span class="number">0</span>))    <span class="keyword">val</span> rowRDD = personRDD.map(_.split(<span class="string">","</span>)).map(attr =&gt; <span class="type">Row</span>(attr(<span class="number">0</span>),attr(<span class="number">1</span>).trim()))    <span class="keyword">val</span> personDF = spark.createDataFrame(rowRDD,schema).show()    <span class="comment">//原有的sparkContext被包在SparkSession里面</span>    <span class="keyword">val</span> file=spark.sparkContext.textFile(args(<span class="number">2</span>))    <span class="keyword">val</span> word=file.flatMap(lines=&gt;lines.split(<span class="string">" "</span>))      .map(word=&gt;(word,<span class="number">1</span>)).reduceByKey(_+_)    word.foreach(println)  }}-----------------------+----------+-------+|  creative|     uv|+----------+-------+|<span class="number">1082303000</span>|<span class="number">7769653</span>||<span class="number">1126317000</span>|<span class="number">5979792</span>||<span class="number">1003303000</span>|<span class="number">5890968</span>||<span class="number">1122319000</span>|<span class="number">5616763</span>||<span class="number">1127317000</span>|<span class="number">5410128</span>||<span class="number">1124303000</span>|<span class="number">5396361</span>||<span class="number">1130317000</span>|<span class="number">5332936</span>||<span class="number">1129317000</span>|<span class="number">5293943</span>||<span class="number">1082317000</span>|<span class="number">5034792</span>||<span class="number">1086517001</span>|<span class="number">4982566</span>|+----------+-------+</code></pre><p>##拓展阅读：<br><a href="http://www.aboutyun.com/thread-25579-1-1.html" target="_blank" rel="noopener">http://www.aboutyun.com/thread-25579-1-1.html</a><br><a href="http://www.cnblogs.com/zzhangyuhang/p/9039695.html" target="_blank" rel="noopener">http://www.cnblogs.com/zzhangyuhang/p/9039695.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在理解spark-session之前让我们理解入口点，一个入口点是控制从操作系统传递到提供的程序的地方。&lt;br&gt;在2.0入口之前，spark-core是sparkContext.Apache Spark是一个功能强大的集群计算引擎，因此它专为快速计算大数据而设计。&lt;br&gt;而
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark安装指南</title>
    <link href="http://yoursite.com/2018/12/11/spark%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/"/>
    <id>http://yoursite.com/2018/12/11/spark安装手册/</id>
    <published>2018-12-11T03:34:48.742Z</published>
    <updated>2019-03-21T03:38:05.080Z</updated>
    
    <content type="html"><![CDATA[<p>在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。<br>一步步你会成功！</p><h2 id="1-Java运行环境"><a href="#1-Java运行环境" class="headerlink" title="1.Java运行环境"></a>1.Java运行环境</h2><h3 id="下载JDK"><a href="#下载JDK" class="headerlink" title="下载JDK"></a>下载JDK</h3><pre><code class="bash">wget --no-cookies --no-check-certificate --header <span class="string">"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie"</span> <span class="string">"http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz"</span>tar -zxvf jdk-8u171-linux-x64.tar.gzmv jdk1.8.0_171/ /usr/<span class="built_in">local</span>/</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/profile--添加以下<span class="built_in">set</span> java environmentexportJAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171exportJRE_HOME=<span class="variable">${JAVA_HOME}</span>/jreexportCLASSPATH=.:<span class="variable">${JAVA_HOME}</span>/lib/dt.JAVA_HOME/lib/tools.jar:<span class="variable">${JRE_HOME}</span>/libexportPATH=<span class="variable">${JAVA_HOME}</span>/bin:<span class="variable">${PATH}</span></code></pre><p>使用命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/profile</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/environmentPATH=<span class="string">"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:<span class="variable">$JAVA_HOME</span>/bin"</span><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><p>输入以下命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/environment</code></pre><h2 id="2-hadoop"><a href="#2-hadoop" class="headerlink" title="2.hadoop"></a>2.hadoop</h2><h3 id="hadoop版本下载"><a href="#hadoop版本下载" class="headerlink" title="hadoop版本下载"></a>hadoop版本下载</h3><p>hadoop-2.7.6.tar.gz</p><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz"</span>``` bash``` bashtar -zxvf hadoop-2.7.6.tar.gz -C /usr/</code></pre><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> HADOOP_HOME=/usr/hadoop-2.7.6<span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbinvim /usr/hadoop-2.7.6/etc/hadoop/hadoop-env.sh<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><h3 id="配置文件设置"><a href="#配置文件设置" class="headerlink" title="配置文件设置"></a>配置文件设置</h3><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/hadoop/tmp&lt;/value&gt;&lt;/property&gt;vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;  &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/hdfs-site.xml&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/datanode&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="hadoop初始化和启动"><a href="#hadoop初始化和启动" class="headerlink" title="hadoop初始化和启动"></a>hadoop初始化和启动</h3><p>初始化hadoop：</p><pre><code class="bash">/usr/hadoop-2.7.6/bin/hdfs namenode -format</code></pre><p>##启动hadoop</p><pre><code class="bash"><span class="variable">$HADOOP_HOME</span>/sbin/start-all.sh</code></pre><h2 id="scala安装"><a href="#scala安装" class="headerlink" title="scala安装"></a>scala安装</h2><pre><code class="bash">wget <span class="string">"https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz"</span>tar -zxvf scala-2.11.12.tgzmv scala-2.11.12/ /usr/<span class="built_in">local</span>/vim /etc/profile<span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:/usr/local/scala-2.11.12/bin"</span></code></pre><h2 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h2><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz"</span>tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C /usr/</code></pre><p>###添加环境变量</p><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7<span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin:<span class="variable">$PATH</span></code></pre><p>###修改spark配置</p><pre><code class="bash">cp /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh.template /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.shvim /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh<span class="built_in">export</span> SPARK_MASTER_IP=localhost<span class="built_in">export</span> SPARK_WORKER_MEMORY=8g <span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171<span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala-2.11.12 <span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7 <span class="built_in">export</span> HADOOP_CONF_DIR=/usr/hadoop-2.7.6/etc/hadoop<span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_HOME</span>/lib <span class="built_in">export</span> SCALA_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span> <span class="built_in">export</span> SPARK_WORKER_CORES=5<span class="built_in">export</span> SPARK_WORKER_INSTANCES=1<span class="built_in">export</span> SPARK_MASTER_PORT=7077</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。&lt;br&gt;一步步你会成功！&lt;/p&gt;
&lt;h2 id=&quot;1-Java运行环境&quot;&gt;&lt;a href=&quot;#1-Java运行环境&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/12/11/hello-world/"/>
    <id>http://yoursite.com/2018/12/11/hello-world/</id>
    <published>2018-12-11T02:52:31.590Z</published>
    <updated>2018-12-09T09:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
