<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jet Smith</title>
  
  
  <link href="/Blog/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-03-21T06:59:10.819Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jet Smith</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark函数:浅析combineByKey、reduceByKey、groupByKey</title>
    <link href="http://yoursite.com/2019/03/21/%E8%AE%A4%E7%9F%A5combineByKey%E3%80%81reduceByKey%E3%80%81groupByKey/"/>
    <id>http://yoursite.com/2019/03/21/认知combineByKey、reduceByKey、groupByKey/</id>
    <published>2019-03-21T06:56:37.000Z</published>
    <updated>2019-03-21T06:59:10.819Z</updated>
    
    <content type="html"><![CDATA[<p>简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。</p><h2 id="combineByKey函数"><a href="#combineByKey函数" class="headerlink" title="combineByKey函数"></a>combineByKey函数</h2><pre><code class="bash">def combineByKey[C](        createCombiner: V =&gt; C,        mergeValue: (C, V) =&gt; C,        mergeCombiners: (C, C) =&gt; C,        partitioner: Partitioner,        mapSideCombine: Boolean = <span class="literal">true</span>,        serializer: Serializer = null)</code></pre><h3 id="三个自定义方法"><a href="#三个自定义方法" class="headerlink" title="三个自定义方法"></a>三个自定义方法</h3><h4 id="1-createCombiner"><a href="#1-createCombiner" class="headerlink" title="1.createCombiner"></a>1.createCombiner</h4><p>这个函数把当前rdd中的值（value）作为参数，此时我们可以对其做些附加操作(类型转换)并把它返回 (这一步类似于初始化操作，分区内操作)</p><h4 id="2-mergeValue"><a href="#2-mergeValue" class="headerlink" title="2. mergeValue"></a>2. mergeValue</h4><p>该函数把元素V合并到之前的元素C(createCombiner)上 (每个分区内合并)</p><h4 id="3-mergeCombiner"><a href="#3-mergeCombiner" class="headerlink" title="3. mergeCombiner"></a>3. mergeCombiner</h4><p>该函数把2个元素C合并 (此函数作用范围在rdd的不同分区间内，跨分区合并)</p><h3 id="使用combineByKey求平均数"><a href="#使用combineByKey求平均数" class="headerlink" title="使用combineByKey求平均数"></a>使用combineByKey求平均数</h3><p>测试代码如下：</p><pre><code class="bash">import org.apache.spark.{SparkConf, SparkContext}object combineByKey {  def main(args: Array[String]): Unit = {    val sparkConf = new SparkConf()      .<span class="built_in">set</span>(<span class="string">"spark.io.compression.codec"</span>, <span class="string">"snappy"</span>)      .setAppName(<span class="string">"dsp_get_request"</span>).setMaster(<span class="string">"local[*]"</span>)    val sc = new SparkContext(sparkConf)    val initialScores = Array((<span class="string">"Fred"</span>, 88.0), (<span class="string">"Fred"</span>, 95.0), (<span class="string">"Fred"</span>, 91.0), (<span class="string">"Wilma"</span>, 93.0), (<span class="string">"Wilma"</span>, 95.0), (<span class="string">"Wilma"</span>, 98.0))    val d1 = sc.parallelize(initialScores)    <span class="built_in">type</span> MVType = (Int, Double) //定义一个元组类型(科目计数器,分数)    val rdd2 = d1.combineByKey(      score =&gt; (1,score),  // 将score映射为一个元组，作为分区内聚合初始值      (c1: MVType , newscore) =&gt; (c1._1 + 1 , c1._2 + newscore),  //分区内聚合      (c1: MVType ,c2: MVType) =&gt; (c1._1 + c2._1,c1._2 + c2._2)  //分区间聚合    ).map{ <span class="keyword">case</span>(name ,(num ,cnt)) =&gt; (name ,num / cnt)}    rdd2.foreach(println)  }}--------------------------------------------------(Fred,91.33333333333333)(Wilma,95.33333333333333)</code></pre><h4 id="使用reduceByKey求平均数"><a href="#使用reduceByKey求平均数" class="headerlink" title="使用reduceByKey求平均数"></a>使用reduceByKey求平均数</h4><p>现在用reduceByKey改写刚才的平均数算法</p><pre><code class="bash">val rdd3 = d1.map(a =&gt; (a._1, (a._2, 1)))  .reduceByKey((a,b) =&gt; (a._1+b._1,a._2+b._2))  .map(t =&gt; (t._1,t._2._1/t._2._2))rdd3.foreach(println)</code></pre><h2 id="reduceByKey、groupByKey"><a href="#reduceByKey、groupByKey" class="headerlink" title="reduceByKey、groupByKey"></a>reduceByKey、groupByKey</h2><p>我们再看看reduceByKey、groupByKey的区别：</p><pre><code class="bash">d1.foreach(println)------------------------(Wilma,93.0)(Fred,95.0)(Fred,88.0)(Fred,91.0)(Wilma,95.0)(Wilma,98.0)d1.reduceByKey(_ + _).foreach(println)-------------------------(Wilma,286.0)(Fred,274.0)d1.groupByKey().foreach(println)-------------------------(Wilma,CompactBuffer(93.0, 95.0, 98.0))(Fred,CompactBuffer(88.0, 95.0, 91.0))</code></pre><h3 id="reduceByKey效率会更高"><a href="#reduceByKey效率会更高" class="headerlink" title="reduceByKey效率会更高"></a>reduceByKey效率会更高</h3><p>其中reduceByKey(_ + _)写法同等于reduceByKey((a,b) =&gt; (a + b))<br>groupByKey会把所有的键值对集合都加载到内存中存储计算，若一个键值对太多，则会导致内存溢出。<br>reduceByKey 进行shuffle之前会在map做合并，这样就会减少shuffle 的IO 传送，效率会高一些。</p><h3 id="groupByKey聚合"><a href="#groupByKey聚合" class="headerlink" title="groupByKey聚合"></a>groupByKey聚合</h3><pre><code class="bash">d1.groupByKey().map(t =&gt; (t._1,t._2.sum)).foreach(println)-------------------------(Fred,274.0)(Wilma,286.0)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单来说reduceByKey、groupByKey都是由combineByKey核心泛型函数实现的，reduceByKey是聚合函数，groupByKey是分组函数，让我们用代码和平均数求解问题，来认识这三个重要的Spark函数。&lt;/p&gt;
&lt;h2 id=&quot;combineB
      
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark安装指南</title>
    <link href="http://yoursite.com/2018/12/11/spark%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C/"/>
    <id>http://yoursite.com/2018/12/11/spark安装手册/</id>
    <published>2018-12-11T03:34:48.742Z</published>
    <updated>2019-03-21T03:38:05.080Z</updated>
    
    <content type="html"><![CDATA[<p>在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。<br>一步步你会成功！</p><h2 id="1-Java运行环境"><a href="#1-Java运行环境" class="headerlink" title="1.Java运行环境"></a>1.Java运行环境</h2><h3 id="下载JDK"><a href="#下载JDK" class="headerlink" title="下载JDK"></a>下载JDK</h3><pre><code class="bash">wget --no-cookies --no-check-certificate --header <span class="string">"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie"</span> <span class="string">"http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz"</span>tar -zxvf jdk-8u171-linux-x64.tar.gzmv jdk1.8.0_171/ /usr/<span class="built_in">local</span>/</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/profile--添加以下<span class="built_in">set</span> java environmentexportJAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171exportJRE_HOME=<span class="variable">${JAVA_HOME}</span>/jreexportCLASSPATH=.:<span class="variable">${JAVA_HOME}</span>/lib/dt.JAVA_HOME/lib/tools.jar:<span class="variable">${JRE_HOME}</span>/libexportPATH=<span class="variable">${JAVA_HOME}</span>/bin:<span class="variable">${PATH}</span></code></pre><p>使用命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/profile</code></pre><p>添加环境变量</p><pre><code class="bash">vim /etc/environmentPATH=<span class="string">"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:<span class="variable">$JAVA_HOME</span>/bin"</span><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib:<span class="variable">$JAVA_HOME</span>/jre/lib<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><p>输入以下命令使环境变量立即生效</p><pre><code class="bash"><span class="built_in">source</span> /etc/environment</code></pre><h2 id="2-hadoop"><a href="#2-hadoop" class="headerlink" title="2.hadoop"></a>2.hadoop</h2><h3 id="hadoop版本下载"><a href="#hadoop版本下载" class="headerlink" title="hadoop版本下载"></a>hadoop版本下载</h3><p>hadoop-2.7.6.tar.gz</p><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.6/hadoop-2.7.6.tar.gz"</span>``` bash``` bashtar -zxvf hadoop-2.7.6.tar.gz -C /usr/</code></pre><h3 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h3><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> HADOOP_HOME=/usr/hadoop-2.7.6<span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbinvim /usr/hadoop-2.7.6/etc/hadoop/hadoop-env.sh<span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171</code></pre><h3 id="配置文件设置"><a href="#配置文件设置" class="headerlink" title="配置文件设置"></a>配置文件设置</h3><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/root/hadoop/tmp&lt;/value&gt;&lt;/property&gt;vim /usr/hadoop-2.7.6/etc/hadoop/core-site.xml&lt;property&gt;  &lt;name&gt;fs.default.name&lt;/name&gt;    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;</code></pre><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/hdfs-site.xml&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.name.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/namenode&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;dfs.data.dir&lt;/name&gt;    &lt;value&gt;/data/hdfs/datanode&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><pre><code class="bash">vim /usr/hadoop-2.7.6/etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt;  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;</code></pre><h3 id="hadoop初始化和启动"><a href="#hadoop初始化和启动" class="headerlink" title="hadoop初始化和启动"></a>hadoop初始化和启动</h3><p>初始化hadoop：</p><pre><code class="bash">/usr/hadoop-2.7.6/bin/hdfs namenode -format</code></pre><p>##启动hadoop</p><pre><code class="bash"><span class="variable">$HADOOP_HOME</span>/sbin/start-all.sh</code></pre><h2 id="scala安装"><a href="#scala安装" class="headerlink" title="scala安装"></a>scala安装</h2><pre><code class="bash">wget <span class="string">"https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz"</span>tar -zxvf scala-2.11.12.tgzmv scala-2.11.12/ /usr/<span class="built_in">local</span>/vim /etc/profile<span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:/usr/local/scala-2.11.12/bin"</span></code></pre><h2 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h2><pre><code class="bash">wget <span class="string">"http://mirror.bit.edu.cn/apache/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz"</span>tar -zxvf spark-2.3.0-bin-hadoop2.7.tgz -C /usr/</code></pre><p>###添加环境变量</p><pre><code class="bash">vim ~/.bashrc<span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7<span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/sbin:<span class="variable">$PATH</span></code></pre><p>###修改spark配置</p><pre><code class="bash">cp /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh.template /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.shvim /usr/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh<span class="built_in">export</span> SPARK_MASTER_IP=localhost<span class="built_in">export</span> SPARK_WORKER_MEMORY=8g <span class="built_in">export</span> JAVA_HOME=/usr/<span class="built_in">local</span>/jdk1.8.0_171<span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala-2.11.12 <span class="built_in">export</span> SPARK_HOME=/usr/spark-2.3.0-bin-hadoop2.7 <span class="built_in">export</span> HADOOP_CONF_DIR=/usr/hadoop-2.7.6/etc/hadoop<span class="built_in">export</span> SPARK_LIBRARY_PATH=<span class="variable">$SPARK_HOME</span>/lib <span class="built_in">export</span> SCALA_LIBRARY_PATH=<span class="variable">$SPARK_LIBRARY_PATH</span> <span class="built_in">export</span> SPARK_WORKER_CORES=5<span class="built_in">export</span> SPARK_WORKER_INSTANCES=1<span class="built_in">export</span> SPARK_MASTER_PORT=7077</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在一台新装机的ubuntu 16上安装java、scala环境，并搭建hadoop、spark节点，并完成基础的配置。&lt;br&gt;一步步你会成功！&lt;/p&gt;
&lt;h2 id=&quot;1-Java运行环境&quot;&gt;&lt;a href=&quot;#1-Java运行环境&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/12/11/hello-world/"/>
    <id>http://yoursite.com/2018/12/11/hello-world/</id>
    <published>2018-12-11T02:52:31.590Z</published>
    <updated>2018-12-09T09:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
